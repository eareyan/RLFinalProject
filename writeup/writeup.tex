%==============================================================================
% Mini Preamble.
%==============================================================================

\documentclass[11pt,twocolumn]{article}
\makeatletter					% Make '@' accessible.
%\pagestyle{myheadings}				% We do our own page headers.
\skip\footins=4ex				% Space above first footnote.
\hbadness=10000					% No "underfull hbox" messages.
\makeatother					% Make '@' special again.
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{subfigure}
\usepackage{float}
%==============================================================================
% Title.
%==============================================================================

\begin{document}
\centerline{\LARGE{CSCI 2951-F Final Project}}
\centerline{Enrique and Ellis and Kavosh and Dave}
\vspace{2mm}


% --- SECTION: Paper Overview ---
\section{Paper Overview}

In this document we survey our efforts to reproduce Figures 6 and 7 from~\cite{jiang2015dependence}. The primary contribution of this paper is to relate the effectiveness of planning depth as model accuracy changes. In particular, they set out to prove, both theoretically and empirically, that with an inaccurate model, planning to a shorter depth can lead to better planning. Their Figure 6 reports results from experiments conducted by running UCT on the RockSample domain and varying the planning depth of UCT. Their Figure 7 reports results from experiments conducted by performing cross validation over possible gammas on a Randomized MDP domain.

Below, we summarize each of these domains, describe our implementation of the experiments and the hypothesize being tested, report our results from these experiments, and survey the ways in which our results and experimental setup differs from the original paper.

% --- SECTION: Domains ---
\section{Domains}

In this section we discussed the two domains used to set up experiments. 
% Section: Rock Sample
\subsection{RockSample}
\label{sec: rocksample}
The RockSample domain consists of an agent acting in a 7 $\times$ 8 GridWorld bounded by walls on the west, south and north sides. There are $k$ rocks that occupy $k$ cells of the grid world where some rocks are good and some rocks are bad. There are $5+k$ actions available to the agent: $North, East, South, West, Sample, \\
Check_1 \ldots Check_k$.

If the agent calls $Sample$ while it is on top of a good rock it receives $+10$ reward and if it does so while on top of a bad rock it receives $-10$ reward. The agent also receives $+10$ reward if it runs off the east edge of the GridWorld, thereby terminating the episode.

The state space is fully observable to the agent except the goodness of rocks -- that is, it always knows its own position and that of all rocks, but not the goodness of the rocks. The $Check_i$ action provides the agent with noisy knowledge of the $i$th rock. If the agent is directly on top of $rock_i$ the $Check$ action returns the true goodness of the rock. As the agent gets further from the rock, the fidelity of its sensor falls off exponentially, bottoming out at a $50\%$ probability of returning the true goodness of the rock. The agent's initial belief state is that each rock has a $50\%$ chance of being good.

% Section: Randomized MDPs
\subsection{Random MDP}
\begin{figure}[H]
\includegraphics[page=1,width=.5\textwidth]{RandomMDPEvaluation.png}
\caption{RandomMDP Evaluation Process}
\end{figure}
The authors designed the and MDP which they called Random MDP. This MDP has 10 states, and 2 actions. 
For each state-action pair, the agent can transition to 5 possible next states chosen uniformly and without replacement.
The transition probability assigned to each next state comes from uniform $(0,1)$ distribution. Rewards are also uniformly distributed between $(0,1)$, however, sample rewards have additive Gaussian noise. To evaluate the MDP there are 2 notions of gamma. The first is $\gamma_{eval} = 0.99$, which is given by the MDP definition. The second is $\gamma_{plan} = 0.3,0.66, \text{and } 0.99$ which was used to plan on this MDP. Figure 1 shows the process by which the authors calculated the loss of a random MDP using both $\gamma_{eval}$ and $\gamma_{plan}$.



% --- SECTION: Experiments ---
\section{Experiments}
In this section we will describe the experiments conducted by the authors.
\subsection{Figure 6}
Figure 6 of the paper tests the hypothesis that a shorter planning depth can lead to better behavior when planning since rollout behavior induces an inaccurate model of the MDP. The rollout algorithm the authors use is Upper Confidence Bound with Trees (UCT), introduced by~\cite{kocsis2006bandit}, a Monte Carlo algorithm which uses confidence bounds in the style of Upper Confidence Bound (UCB), introduced by~\cite{auer2002finite}.

Experiments were conducted on the RockSample domain described in Section \ref{sec: rocksample}. We implemented a RockSample domain and code\footnote{https://github.com/eareyan/RLFinalProject.} to generate our plots using the Brown-UMBC (BURLAP)\footnote{http://burlap.cs.brown.edu/}.

\subsection{Figure 7}

The purpose of this figure is to make two points: 1) with a model estimated from insufficient experience size it is better to use a planning horizon shorter than the true horizon and 2) performing a cross validation is a reasonable method to find a good gamma relative to the amount of experience. 
% --- SECTION: Results ---
\section{Results}
In this section we report the results we get while reproducing the experiments.

\subsection{UCT Performance vs. Planning Depth on Rock Sample}
Our UCT results roughly capture the same trend that those of \cite{jiang2015dependence}. The trend which we hope to show is an increase in planning efficacy using an intermediate -- rather than very low or very high -- planning depth for UCT. Two of our three curves, UCT with 50 and 200 trajectories, demonstrated exactly this trend while our third curve, UCT with 1000 trajectories, performed equally well with intermediate and high planning depths. See Figure \ref{fig: UCTOurResults} for complete results. We conjecture that the discrepancies in our results are symptoms of the differences in our experimental setup -- see Section \ref{sec: repDisc} for more detail.

\begin{figure}[H]
\centering
\subfigure[Our results]{
\label{fig: UCTOurResults}
\includegraphics[page=1,width=.23\textwidth]{../results/rock_sample_results.png}}
\hspace{1mm}
\subfigure[Their results]{
\includegraphics[page=1,width=.22\textwidth]{../results/figure_6.png}}
\caption{A comparison of our results and their results for UCT Experiments.}
\end{figure}

% Subsection: Cross validation with RandomMDP
\subsection{Cross validation with RandomMDP}

Our attempts to reproduce figure 7 of the original paper is shown in figure 3 of the present document. 
\begin{figure}[H]
\centering
\subfigure[Our results]{
\includegraphics[page=1,width=.20\textwidth]{../results/figure_2.pdf}}
\hspace{4mm}
\subfigure[Their results]{
\includegraphics[page=1,width=.20\textwidth]{../results/originalCV.pdf}}
\caption{A comparison of our results and their results for Random MDP Experiments.}
\end{figure}

The overall shape of our results reflect that of the paper. Notice that with fewer amount of experience is better to use a lower gamma while using a higher gamma is preferred with sufficient experience. Also, notice that cross validation can offer a good performance by setting gamma as a function of the amount of experience. 
% --- SECTION: Reproducibility Discussion ---
\section{Reproducibility Discussion}
\label{sec: repDisc}

There were a number of ambiguities in \cite{jiang2015dependence} as well as computational constraints placed on us which prevented us from perfectly recreating the results of the paper. 

\subsection{Ambiguity about RockSample}
The initial state of the agent was underspecified. It was not stated whether the same initial state was used in each episode or if the state was initially randomized. In the case of the former, it was not clear what the initial state would be and in the case of the latter the distribution over states and number of rocks was indeterminate. We consulted the authors on this and determined that the same initial state with 8 rocks at fixed locations was used. However, we used only 2 rocks at fixed locations to reduce computational load. Similarly, the initial goodness of rocks was not specified -- we assumed all rocks are initially good.

There was additional ambiguity regarding the the fidelity falloff of the $Check$ actions. We assumed the distance between the agent and a rock (which determines how unreliable the sensor is) is measured using Euclidean distance. It was also unclear what the decay rate for exponential falloff was; we used $\frac{1}{2}$.

\subsection{Ambiguity about UCT}

Running UCT involves assigning several parameters that can dramatically alter results. For instance, the assignment of the exploration bias (i.e. the UCB parameter) will affect how UCT plans. The paper reports that they assigned this parameter for each datapoint by optimizing over the set $10 * \exp{-2, -1, 0, 1, 2}$. We fixed this parameter to be the maximum, $10 \cdot \exp(2) \approx 74$ for all experiments since we lacked the computational resources required to optimize in this way (see Section \ref{sec: compLim}).

Additionally, the value of the discount factor $\gamma$ is critical in determining behavior, especially in Rock Sample where there are large bursts of reward in the future. We set $\gamma$ to be $0.9$ for our experiments, but after communicating with the authors, discovered there setting of $\gamma$ was 0.99 (although they were unsure). 

Lastly, there are several different ways one could imagine running UCT on a POMDP. We chose to convert the POMDP into a BeliefMDP and run UCT on the BeliefMDP, both during simulation and evaluation. The authors chose to sample ground states from their belief state during simulation but evaluate the performance of UCT by explicitly tracking the belief state using a generative model of the ground MDP.

\subsection{General Computational Limits in RockSample}
\label{sec: compLim}
The full barrage of experiments used by \cite{jiang2015dependence} is extremely computationally taxing. They ran $\sim$10,000 trials per data point, with very high number of UCT trajectories and 8 rocks which leads to $\sim$16,000 states over which belief state updates must be performed. 

We were forced to dramatically reduce the complexity of these parameters. We ran 10 trials per data point, with 100$\times$ fewer trajectories with only 2 rocks which leads to ~200 states over which belief state updates must be performed. Even after a series of optimizations for BURLAP, it still took over 5 hours to fully gather our data.


\section{Personal communication with the authors}
In order to clarify some of the implementation details, we communicated with the authors via email. 
The authors were very receptive to our communications and answer questions to the best of their abilities. In particular, they sent their code (written in Matlab) that they used
to generate results. Unfortunately the code was very hard to read and in some cases it raised even more questions that it answered. 

% --- BIBLIOGRAPHY ---
\bibliographystyle{unsrt}
\bibliography{lsdm_final}

\end{document}